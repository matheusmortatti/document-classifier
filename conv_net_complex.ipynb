{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scikit-image --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Preprocess image, normalizing and resizing it\n",
    "\n",
    "    :param frame: RGBA frame\n",
    "\"\"\"    \n",
    "def preprocess_image(frame):\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = frame/255.0 - 0.5\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, IMAGE_PP_SIZE)\n",
    "    \n",
    "    return preprocessed_frame\n",
    "\n",
    "\"\"\"\n",
    "    Create 2D label list from 1D list\n",
    "    \n",
    "    :param labels: 1D label list\n",
    "\"\"\"\n",
    "\n",
    "def make_labels(labels):\n",
    "    np_labels = np.zeros((len(labels), 16))\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        np_labels[i, labels[i]] = 1\n",
    "    \n",
    "    return np_labels\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "def choices(l, k=1):\n",
    "    new_list = []\n",
    "    for i in range(k):\n",
    "        new_list.append(random.choice(l))\n",
    "    return new_list\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(IMAGE_PP_SIZE[0], IMAGE_PP_SIZE[1], 1)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(16, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PP_SIZE = [150, 150]\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100\n",
    "TRAIN_STEP = 1000\n",
    "VAL_SIZE = 100\n",
    "\n",
    "RELOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model if it Exists, Otherwise Recreate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RELOAD_MODEL:\n",
    "    try:\n",
    "        json_file = open('model_complex.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        model.load_weights(\"model_complex.h5\")\n",
    "    except FileNotFoundError:\n",
    "        model = build_model()\n",
    "else:\n",
    "    model = build_model()\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusmortatti/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/matheusmortatti/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6594 - acc: 0.8060 - val_loss: 0.6052 - val_acc: 0.7900\n",
      "trained: 1000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6713 - acc: 0.7960 - val_loss: 0.8252 - val_acc: 0.7300\n",
      "trained: 2000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6383 - acc: 0.8170 - val_loss: 0.6209 - val_acc: 0.7800\n",
      "trained: 3000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6822 - acc: 0.7970 - val_loss: 0.6286 - val_acc: 0.8100\n",
      "trained: 4000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6508 - acc: 0.8010 - val_loss: 0.7418 - val_acc: 0.7600\n",
      "trained: 5000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6813 - acc: 0.8130 - val_loss: 0.6290 - val_acc: 0.7700\n",
      "trained: 6000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6728 - acc: 0.7980 - val_loss: 0.7737 - val_acc: 0.8200\n",
      "trained: 7000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7178 - acc: 0.7830 - val_loss: 0.6406 - val_acc: 0.8200\n",
      "trained: 8000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6650 - acc: 0.8060 - val_loss: 0.8274 - val_acc: 0.7500\n",
      "trained: 9000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7302 - acc: 0.7910 - val_loss: 1.0316 - val_acc: 0.7500\n",
      "trained: 10000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6984 - acc: 0.7770 - val_loss: 0.8094 - val_acc: 0.7600\n",
      "trained: 11000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6465 - acc: 0.7970 - val_loss: 0.7879 - val_acc: 0.7700\n",
      "trained: 12000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6681 - acc: 0.8020 - val_loss: 0.5931 - val_acc: 0.8400\n",
      "trained: 13000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6531 - acc: 0.7960 - val_loss: 0.5295 - val_acc: 0.8400\n",
      "trained: 14000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6323 - acc: 0.8060 - val_loss: 0.8357 - val_acc: 0.7300\n",
      "trained: 15000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6819 - acc: 0.7930 - val_loss: 0.5640 - val_acc: 0.8100\n",
      "trained: 16000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5694 - acc: 0.8290 - val_loss: 0.8366 - val_acc: 0.7500\n",
      "trained: 17000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6123 - acc: 0.8360 - val_loss: 0.9985 - val_acc: 0.7100\n",
      "trained: 18000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5643 - acc: 0.8290 - val_loss: 0.8315 - val_acc: 0.7200\n",
      "trained: 19000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7394 - acc: 0.7820 - val_loss: 0.7841 - val_acc: 0.7900\n",
      "trained: 20000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6765 - acc: 0.7900 - val_loss: 0.5886 - val_acc: 0.8100\n",
      "trained: 21000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6762 - acc: 0.8040 - val_loss: 0.7446 - val_acc: 0.7800\n",
      "trained: 22000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6612 - acc: 0.8180 - val_loss: 0.6442 - val_acc: 0.8200\n",
      "trained: 23000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5735 - acc: 0.8400 - val_loss: 0.9479 - val_acc: 0.7400\n",
      "trained: 24000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6035 - acc: 0.8170 - val_loss: 0.9727 - val_acc: 0.7500\n",
      "trained: 25000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7089 - acc: 0.7970 - val_loss: 0.9187 - val_acc: 0.7300\n",
      "trained: 26000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6145 - acc: 0.8220 - val_loss: 0.8691 - val_acc: 0.7600\n",
      "trained: 27000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6429 - acc: 0.8120 - val_loss: 0.7361 - val_acc: 0.7400\n",
      "trained: 28000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6417 - acc: 0.8130 - val_loss: 0.8215 - val_acc: 0.7500\n",
      "trained: 29000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6569 - acc: 0.8050 - val_loss: 0.7921 - val_acc: 0.7600\n",
      "trained: 30000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6825 - acc: 0.8030 - val_loss: 1.0161 - val_acc: 0.7100\n",
      "trained: 31000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6138 - acc: 0.8190 - val_loss: 0.8603 - val_acc: 0.7300\n",
      "trained: 32000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6360 - acc: 0.8010 - val_loss: 0.8450 - val_acc: 0.7200\n",
      "trained: 33000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6221 - acc: 0.8000 - val_loss: 0.7712 - val_acc: 0.7200\n",
      "trained: 34000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6427 - acc: 0.8020 - val_loss: 0.7322 - val_acc: 0.7900\n",
      "trained: 35000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6887 - acc: 0.7930 - val_loss: 0.6659 - val_acc: 0.7800\n",
      "trained: 36000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6950 - acc: 0.8020 - val_loss: 0.7211 - val_acc: 0.8000\n",
      "trained: 37000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6575 - acc: 0.8080 - val_loss: 0.7802 - val_acc: 0.7500\n",
      "trained: 38000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5845 - acc: 0.8250 - val_loss: 0.7210 - val_acc: 0.7700\n",
      "trained: 39000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6667 - acc: 0.8120 - val_loss: 0.6429 - val_acc: 0.7900\n",
      "trained: 40000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5925 - acc: 0.8280 - val_loss: 0.6945 - val_acc: 0.7900\n",
      "trained: 41000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6521 - acc: 0.8070 - val_loss: 0.6061 - val_acc: 0.7800\n",
      "trained: 42000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6699 - acc: 0.8100 - val_loss: 0.6760 - val_acc: 0.8000\n",
      "trained: 43000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6224 - acc: 0.8110 - val_loss: 0.4628 - val_acc: 0.8500\n",
      "trained: 44000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7047 - acc: 0.7810 - val_loss: 0.6890 - val_acc: 0.7800\n",
      "trained: 45000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6540 - acc: 0.8100 - val_loss: 0.7087 - val_acc: 0.8100\n",
      "trained: 46000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6403 - acc: 0.8110 - val_loss: 0.7128 - val_acc: 0.7800\n",
      "trained: 47000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6584 - acc: 0.7940 - val_loss: 1.0216 - val_acc: 0.7200\n",
      "trained: 48000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6288 - acc: 0.8110 - val_loss: 1.0393 - val_acc: 0.6800\n",
      "trained: 49000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6432 - acc: 0.8180 - val_loss: 0.6573 - val_acc: 0.8200\n",
      "trained: 50000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6310 - acc: 0.8240 - val_loss: 0.5869 - val_acc: 0.8500\n",
      "trained: 51000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6771 - acc: 0.7980 - val_loss: 0.9343 - val_acc: 0.7500\n",
      "trained: 52000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6890 - acc: 0.7970 - val_loss: 0.6771 - val_acc: 0.7700\n",
      "trained: 53000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6093 - acc: 0.8250 - val_loss: 0.7276 - val_acc: 0.7400\n",
      "trained: 54000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6068 - acc: 0.8190 - val_loss: 0.8540 - val_acc: 0.7500\n",
      "trained: 55000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6375 - acc: 0.8110 - val_loss: 0.7469 - val_acc: 0.8200\n",
      "trained: 56000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6335 - acc: 0.8110 - val_loss: 0.9512 - val_acc: 0.7200\n",
      "trained: 57000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6172 - acc: 0.8220 - val_loss: 0.7484 - val_acc: 0.7400\n",
      "trained: 58000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6715 - acc: 0.7940 - val_loss: 0.9067 - val_acc: 0.7300\n",
      "trained: 59000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6470 - acc: 0.7920 - val_loss: 0.6728 - val_acc: 0.7900\n",
      "trained: 60000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6458 - acc: 0.8140 - val_loss: 0.7534 - val_acc: 0.8000\n",
      "trained: 61000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6286 - acc: 0.8070 - val_loss: 0.7366 - val_acc: 0.7800\n",
      "trained: 62000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6518 - acc: 0.7930 - val_loss: 0.7845 - val_acc: 0.7600\n",
      "trained: 63000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6911 - acc: 0.8020 - val_loss: 0.6851 - val_acc: 0.8000\n",
      "trained: 64000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6289 - acc: 0.8110 - val_loss: 0.9275 - val_acc: 0.7300\n",
      "trained: 65000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6169 - acc: 0.8130 - val_loss: 0.8879 - val_acc: 0.7600\n",
      "trained: 66000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6285 - acc: 0.8030 - val_loss: 1.0110 - val_acc: 0.7400\n",
      "trained: 67000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6277 - acc: 0.8080 - val_loss: 0.8057 - val_acc: 0.7900\n",
      "trained: 68000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6164 - acc: 0.8190 - val_loss: 1.0208 - val_acc: 0.6500\n",
      "trained: 69000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5758 - acc: 0.8250 - val_loss: 0.7333 - val_acc: 0.7800\n",
      "trained: 70000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6052 - acc: 0.8170 - val_loss: 0.7212 - val_acc: 0.8000\n",
      "trained: 71000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5808 - acc: 0.8180 - val_loss: 0.8378 - val_acc: 0.7800\n",
      "trained: 72000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6568 - acc: 0.7960 - val_loss: 0.7964 - val_acc: 0.7600\n",
      "trained: 73000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6333 - acc: 0.8100 - val_loss: 0.7561 - val_acc: 0.7600\n",
      "trained: 74000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6436 - acc: 0.8130 - val_loss: 0.6339 - val_acc: 0.8500\n",
      "trained: 75000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6457 - acc: 0.8060 - val_loss: 0.9692 - val_acc: 0.6900\n",
      "trained: 76000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6396 - acc: 0.8100 - val_loss: 0.9086 - val_acc: 0.7400\n",
      "trained: 77000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7046 - acc: 0.7900 - val_loss: 0.7803 - val_acc: 0.7500\n",
      "trained: 78000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6049 - acc: 0.8240 - val_loss: 0.6733 - val_acc: 0.7600\n",
      "trained: 79000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6408 - acc: 0.8030 - val_loss: 0.5408 - val_acc: 0.8100\n",
      "trained: 80000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7054 - acc: 0.7940 - val_loss: 0.7881 - val_acc: 0.7300\n",
      "trained: 81000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6016 - acc: 0.8180 - val_loss: 0.7108 - val_acc: 0.7900\n",
      "trained: 82000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5890 - acc: 0.8180 - val_loss: 0.7585 - val_acc: 0.7800\n",
      "trained: 83000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6546 - acc: 0.8110 - val_loss: 0.6904 - val_acc: 0.8300\n",
      "trained: 84000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7016 - acc: 0.7880 - val_loss: 0.9423 - val_acc: 0.7700\n",
      "trained: 85000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6694 - acc: 0.8030 - val_loss: 0.8581 - val_acc: 0.7500\n",
      "trained: 86000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6183 - acc: 0.8240 - val_loss: 0.7799 - val_acc: 0.7500\n",
      "trained: 87000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6088 - acc: 0.8270 - val_loss: 0.6662 - val_acc: 0.7800\n",
      "trained: 88000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6694 - acc: 0.7910 - val_loss: 0.7654 - val_acc: 0.7600\n",
      "trained: 89000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5982 - acc: 0.8090 - val_loss: 0.7544 - val_acc: 0.7600\n",
      "trained: 90000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5462 - acc: 0.8370 - val_loss: 0.8439 - val_acc: 0.7500\n",
      "trained: 91000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6828 - acc: 0.7910 - val_loss: 0.7513 - val_acc: 0.7800\n",
      "trained: 92000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5593 - acc: 0.8300 - val_loss: 0.7225 - val_acc: 0.7800\n",
      "trained: 93000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5993 - acc: 0.8170 - val_loss: 0.6891 - val_acc: 0.8100\n",
      "trained: 94000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6779 - acc: 0.7860 - val_loss: 1.0564 - val_acc: 0.6900\n",
      "trained: 95000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6834 - acc: 0.7890 - val_loss: 0.7721 - val_acc: 0.7400\n",
      "trained: 96000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6717 - acc: 0.8160 - val_loss: 0.8773 - val_acc: 0.7500\n",
      "trained: 97000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6350 - acc: 0.8030 - val_loss: 0.8955 - val_acc: 0.7000\n",
      "trained: 98000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5814 - acc: 0.8180 - val_loss: 0.6568 - val_acc: 0.8300\n",
      "trained: 99000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6588 - acc: 0.7890 - val_loss: 0.7743 - val_acc: 0.7900\n",
      "trained: 100000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5957 - acc: 0.8170 - val_loss: 0.8549 - val_acc: 0.7200\n",
      "trained: 101000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6377 - acc: 0.7930 - val_loss: 0.7340 - val_acc: 0.8000\n",
      "trained: 102000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6647 - acc: 0.8070 - val_loss: 0.8627 - val_acc: 0.7000\n",
      "trained: 103000 / 320000\n",
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6505 - acc: 0.7880 - val_loss: 0.8819 - val_acc: 0.7400\n",
      "trained: 104000 / 320000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d50745410500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mpp_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/images/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpp_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, flatten, **plugin_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                                (plugin, kind))\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/skimage/io/_plugins/tifffile_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, dtype, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'img_num'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img_num'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mtif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTiffFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#\n",
    "# Define train, validation and test lists\n",
    "#\n",
    "\n",
    "train_labels = []\n",
    "val_labels = []\n",
    "test_labels = []\n",
    "\n",
    "train_input = []\n",
    "val_input = []\n",
    "test_input = []\n",
    "\n",
    "#\n",
    "# Load validation dataset\n",
    "#\n",
    "\n",
    "folder = \"/media/matheusmortatti/External/rvl-cdip\"\n",
    "labels_file = open(folder + \"/labels/val.txt\")\n",
    "\n",
    "val_file = []\n",
    "for line in labels_file:\n",
    "    sp = line.split()\n",
    "    val_file.append(sp)\n",
    "    \n",
    "#\n",
    "# Open training list file and start training\n",
    "#\n",
    "\n",
    "\n",
    "trained = 0\n",
    "train_size = file_len(folder + \"/labels/train.txt\")\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    trained = 0\n",
    "    labels_file = open(folder + \"/labels/train.txt\")\n",
    "    for line in labels_file:\n",
    "        sp = line.split()\n",
    "        pp_img = preprocess_image(imread(folder + \"/images/\" + sp[0]))\n",
    "\n",
    "        train_input.append(pp_img)\n",
    "        train_labels.append(int(sp[1]))\n",
    "\n",
    "        if len(train_input) >= TRAIN_STEP:\n",
    "            \n",
    "            trained += len(train_input)\n",
    "\n",
    "            #\n",
    "            # Choose a subset of the validation data\n",
    "            #\n",
    "\n",
    "            ss_val = choices(val_file, k=VAL_SIZE)\n",
    "\n",
    "            for v in ss_val:\n",
    "                val_input.append(preprocess_image(imread(folder + \"/images/\" + v[0])))\n",
    "                val_labels.append(int(v[1]))\n",
    "\n",
    "            x_val = np.expand_dims(np.asarray(val_input), axis=3)\n",
    "            y_val = make_labels(val_labels)\n",
    "\n",
    "            #\n",
    "            # Create training data\n",
    "            #\n",
    "\n",
    "            x_train = np.expand_dims(np.asarray(train_input), axis=3)\n",
    "            y_train = make_labels(train_labels)\n",
    "\n",
    "            model.fit(x_train, y_train,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=1,\n",
    "                      verbose=1,\n",
    "                      validation_data=(x_val, y_val))\n",
    "\n",
    "            #\n",
    "            # Reset lists for next iteration\n",
    "            #\n",
    "\n",
    "            train_input = []\n",
    "            train_labels = []\n",
    "            val_input = []\n",
    "            val_labels = []\n",
    "            \n",
    "            print(\"trained: \" + str(trained) + \" / \" + str(train_size))\n",
    "    labels_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model_complex.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_complex.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_img_type = np.zeros((len(img_type), dt))\n",
    "\n",
    "print(len(img_type))\n",
    "\n",
    "for i in range(len(img_type)):\n",
    "    np_img_type[i, img_type[i]] = 1\n",
    "print(np_img_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(np.asarray(imgs[:2000]), axis=3)\n",
    "y_train = np_img_type[:2000,:]\n",
    "\n",
    "x_val = np.expand_dims(np.asarray(imgs[2000:4000]), axis=3)\n",
    "y_val = np_img_type[2000:4000,:]\n",
    "\n",
    "x_test = np.expand_dims(np.asarray(imgs[4000:]), axis=3)\n",
    "y_test = np_img_type[4000:,:]\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=2,\n",
    "          verbose=1,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Y_pred\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
