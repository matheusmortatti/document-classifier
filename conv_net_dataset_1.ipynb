{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scikit-image --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import png\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Preprocess image, normalizing and resizing it\n",
    "\n",
    "    :param frame: RGBA frame\n",
    "\"\"\"    \n",
    "def preprocess_image(frame):\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = frame/255.0 - 0.5\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [150,150])\n",
    "    \n",
    "    return preprocessed_frame\n",
    "\n",
    "def make_labels(labels):\n",
    "    np_labels = np.zeros((len(labels), 16))\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        np_labels[i, labels[i]] = 1\n",
    "    \n",
    "    return np_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(20, kernel_size=(7, 7), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(150, 150, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), strides=(1, 1)))\n",
    "model.add(Conv2D(50, kernel_size = (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 33s 326ms/step - loss: 2.6485 - acc: 0.1700 - val_loss: 2.7781 - val_acc: 0.1100\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 2.7137 - acc: 0.1000 - val_loss: 2.7540 - val_acc: 0.1500\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 2.7562 - acc: 0.0700 - val_loss: 2.7394 - val_acc: 0.1500\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 30s 297ms/step - loss: 2.7293 - acc: 0.1300 - val_loss: 2.7207 - val_acc: 0.1600\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 2.7233 - acc: 0.1100 - val_loss: 2.7080 - val_acc: 0.1700\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 2.6907 - acc: 0.0800 - val_loss: 2.6990 - val_acc: 0.2100\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 2.6471 - acc: 0.1700 - val_loss: 2.6731 - val_acc: 0.1900\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 2.7054 - acc: 0.0800 - val_loss: 2.6605 - val_acc: 0.2400\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 2.6403 - acc: 0.1600 - val_loss: 2.6417 - val_acc: 0.2000\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 28s 279ms/step - loss: 2.6257 - acc: 0.1400 - val_loss: 2.6762 - val_acc: 0.1700\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 2.6646 - acc: 0.1800 - val_loss: 2.6454 - val_acc: 0.1200\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 2.5818 - acc: 0.1400 - val_loss: 2.5792 - val_acc: 0.1600\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 2.5830 - acc: 0.2000 - val_loss: 2.5297 - val_acc: 0.2500\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 2.5374 - acc: 0.2300 - val_loss: 2.4825 - val_acc: 0.2600\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      " 10/100 [==>...........................] - ETA: 18s - loss: 2.3628 - acc: 0.3000"
     ]
    }
   ],
   "source": [
    "train_labels = []\n",
    "val_labels = []\n",
    "test_labels = []\n",
    "\n",
    "train_input = []\n",
    "val_input = []\n",
    "test_input = []\n",
    "\n",
    "folder = \"./data/rvl-cdip\"\n",
    "labels_file = open(folder + \"/labels/val.txt\")\n",
    "\n",
    "for line in labels_file:\n",
    "    sp = line.split()\n",
    "    pp_img = preprocess_image(imread(folder + \"/images/\" + sp[0]))\n",
    "    \n",
    "    val_input.append(pp_img)\n",
    "    val_labels.append(int(sp[1]))\n",
    "    \n",
    "    if len(val_input) >= 100:\n",
    "        break\n",
    "\n",
    "x_val = np.expand_dims(np.asarray(val_input), axis=3)\n",
    "y_val = make_labels(val_labels)\n",
    "\n",
    "labels_file = open(folder + \"/labels/train.txt\")\n",
    "\n",
    "for line in labels_file:\n",
    "    sp = line.split()\n",
    "    pp_img = preprocess_image(imread(folder + \"/images/\" + sp[0]))\n",
    "    \n",
    "    train_input.append(pp_img)\n",
    "    train_labels.append(int(sp[1]))\n",
    "    \n",
    "    if len(train_input) >= 100:\n",
    "        x_train = np.expand_dims(np.asarray(train_input), axis=3)\n",
    "        y_train = make_labels(train_labels)\n",
    "        \n",
    "        \n",
    "        model.fit(x_train, y_train,\n",
    "                  batch_size=10,\n",
    "                  epochs=1,\n",
    "                  verbose=1,\n",
    "                  validation_data=(x_val, y_val))\n",
    "        \n",
    "        train_input = []\n",
    "        train_labels = []\n",
    "        \n",
    "#     path = sp[0].split('/')\n",
    "#     pp_path = folder + \"/images_pp/\" + \"/\".join(path[:-1])\n",
    "#     if not os.path.exists(pp_path):\n",
    "#         os.makedirs(pp_path)\n",
    "#     pp_path = pp_path + '/' + str(path[-1].split('.')[0]) + \"_pp\"\n",
    "#     np.save(pp_path, pp_img)\n",
    "    \n",
    "#     train_path.append(pp_path)\n",
    "#     train_labels.append(sp[1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_img_type = np.zeros((len(img_type), dt))\n",
    "\n",
    "print(len(img_type))\n",
    "\n",
    "for i in range(len(img_type)):\n",
    "    np_img_type[i, img_type[i]] = 1\n",
    "print(np_img_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(np.asarray(imgs[:2000]), axis=3)\n",
    "y_train = np_img_type[:2000,:]\n",
    "\n",
    "x_val = np.expand_dims(np.asarray(imgs[2000:4000]), axis=3)\n",
    "y_val = np_img_type[2000:4000,:]\n",
    "\n",
    "x_test = np.expand_dims(np.asarray(imgs[4000:]), axis=3)\n",
    "y_test = np_img_type[4000:,:]\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=2,\n",
    "          verbose=1,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Y_pred\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
